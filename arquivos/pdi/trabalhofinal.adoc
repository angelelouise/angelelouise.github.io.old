= APLICATIVO ANDROID DE DETECÇÃO E RASTREAMENTO DE FACE HUMANA EM TEMPO REAL UTILIZANDO OPENCV
:Author:    Victor Mafra e Angele Louise
:Email:     <angelealst@hotmail.com e vick.vems@gmail.com>
:toc: left

== 1. INTRODUÇÃO
O seguinte trabalho trata sobre experiência realizada em plataforma Android com finalidade de desenvolver um aplicativo para detecção e rastreamento de faces em tempo real utilizando OpenCV(biblioteca multiplataforma de computação visual) para o processamento de vídeo. Para detectar faces foi feito uso de classificadores do tipo LBP e Haar e para o rastreamento o método de Lucas-Kanade, que são recursos oferecidos pela biblioteca supracitada. 
Tal trabalho foi feito com o objetivo de adquirir conhecimento na área assim como atender as demandas acadêmicas do terceiro período da disciplina de processamento digital de imagens em 2016.1 do curso de Engenharia de Computação/UFRN.


== 2. PROGRAMAS E FERRAMENTAS UTILIZADAS

O ambiente de desenvolvimento utilizado foi o Android Studio versão 1.5.1, que é o oficial para desenvolvimento em Android. Para o desenvolvimento das funções de reconhecimento e rastreio foram usadas as bibliotecas do OpenCV(API de visão computacional) na versão 2.4.10 baixadas do site. Para o funcionamento do aplicativo no dispositivos foi necessário baixar do Google Play o OpenCV Manager, que são as bibliotecas necessárias para o funcionamento correto dos aplicativos publicados e que fazem uso de funções do OpenCV. Essa dependência existe visando o menor tamanho dos aplicativos, de tal forma que nenhum deles carregam as bibliotecas em seu código. Os testes foram realizados em 3 dispositivos: Tablet Nexus 7, Lenovo A7010 e Morotola Moto G 2ªgen.
    
== 3. CONTEÚDO TEÓRICO

O conteúdo desenvolvido no experimento aborda algoritmos já conhecidos para detecção de faces e rastreio. De modo que foram utilizados classificadores do tipo LBP e Haar, assim como método da pirâmide para rastreamento, essa sessão tratara acerca desses algoritmos.

=== 3.1. ALGORITMO DE VIOLA-JONES

É o primeiro que oferece taxas de detecção de objetos competitivas em tempo real, proposto em 2001 por Paul Viola e Michael Jones. Apesar do algoritmo ter capacidade de receber treinamento para detectar qualquer objeto, seu principal objetivo atualmente é a detecção de faces.
O algoritmo possui uma característica importante, ele apenas distingue as faces das não-faces, ele não as reconhece. Outras características importantes são: a sua robustez, possui uma alta taxa de positivos e uma baixa taxa de falsos positivos e ocorre em tempo real. 

O algoritmo possui 4 estágios:

==== 3.1.1. Seleção Haar feature:

Este método utiliza-se de unidades chamadas _features_ retangulares, eles possuem quatro padrões possíveis e formatos específicos, entretanto podem possuir dimensões e posições arbitrárias dentro de uma janela também arbitrária, o calculo do valor dessas unidades se dá pela diferença entre as somatórias dos _pixels_ na região preta e dos _pixels_ na região branca.

.Configuração de uma feature.
image::imagens_final/feature.jpg[]
 
*Haar features* partem do pressuposto que todas as faces compartilham de propriedades similares, são elas: A região dos olhos, que é mais escura que a região acima das bochechas; A região do nariz, que é mais clara que a região dos olhos; Localização dos olhos, nariz e boca por proporção.

.Sobreposição de features em uma face.
image::imagens_final/face.jpg[]

==== 3.1.2. Criação de uma imagem integral:

Para acelerar o cálculo do valor de um feature sobre uma imagem, é necessário usá-la na representação integral, que é o cálculo do somatório de todos os pixels ao redor de um pixel em específico, como pode ser visto na imagem abaixo.

.Imagem integral
image::imagens_final/imagem.jpg[]

O valor da imagem integral no ponto 1 é a soma dos pixels de A, enquanto que o ponto 2 é A+B, o ponto 3 é A+C e o ponto 4 é A+B+C+D, portanto a soma dos valores dos pixels do retângulo D pode ser calculada por (4+1) - (2+3).

==== 3.1.3. Treinamento AdaBoost:

O aprendizado baseado em AdaBoost é utilizado para definir quais das inúmeras features serão utilizadas e também os valores dos limiares a que estarão sujeitas. Esse algoritmo de aprendizado constrói um classificador poderoso através da combinação linear de vários classificadores simples “fracos” ponderados, tal estrutura minimiza a ocorrência de falsos negativos.

==== 3.1.4. Classificadores em cascata:

Estes classificadores são distribuídos em forma de cascata, onde cada nível possui uma quantidade de classificadores fortes. Seu uso torna a detecção de não-faces mais rápida, acelerando a execução do algoritmo.

.Estrutura que representa o detector em cascata.
image::imagens_final/cascata.jpg[]

=== 3.2. CLASSIFICADOR LBP e Haar-like

O LBP e Haar-like são descritores de textura e forma, que ao serem combinados com classificadores de modo sequencial proporcionam um classificador com baixa taxa de erro.
Feições Haar-like, como visto no tópico anterior, são filtros em que se deve subtrair as regiões positivas (brancas) das negativas (pretas) para se obter um valor que é posteriormente utilizado na categorização das sub-regiões de uma determinada imagem.
LBP ou Local Binary Pattern, define a textura como uma função de variação espacial na intensidade dos pixels de uma imagem, a idéia desse operador é que feições comuns podem ser representadas através de um valor de uma determinada escala númerica, portanto com um conjunto de valores extraídos é possível fazer o reconhecimento de um determinado objeto em uma imagem.
O treinamento e classificação de ambas as abordagens são similares.

.Fase de classificação das abordagens Haar e LBP em cascata.
image::imagens_final/LBP.jpg[]

=== 3.3. MÉTODO LUCAS-KANADE

Foi desenvolvido por Bruce D. Lucas e Takeo Kanade em 1981, é um método diferencial usado para estimativa de fluxo óptico.
Ele avalia a movimentação de pontos em uma pequena janela ao redor do ponto de interesse. É um método não interativo que assume um fluxo óptico constante local, constância de brilho, pequenos movimentos e coerência espacial. Sua abordagem consiste em dividir a imagem em janelas e calcular se os pixels deslocaram de lugar entre as janelas.

== 4. ESTRUTURA DE UM APLICATIVO ANDROID

Há pelo menos 3 camadas distintas que funcionam juntas para o funcionamento do aplicativo. Dentre os três, dois são arquivos xml. O primeiro arquivo xml, chamado **manifest**, é responsável por informar ao sistema quais os componentes do aplicativo e quais as permissões que o mesmo pode ter junto ao sistema. Por exemplo, o trecho *<uses-permission android:name="android.permission.CAMERA"/>* da permissão ao aplicativo de fazer uso das câmeras do dispositivo. O segundo arquivo xml é o de Layout, que respalda as componentes do aplicativo e é responsável pela configuração e organização dos elementos de interação com o usuário. É possível construir o layout inteiramente através de código no arquivo xml com mecanismos de ajuda como autocomplete, assim como o mesmo pode ser suprimido e a codificação ser feita na componente principal em Java. Há ainda a possibilidade de montar o layout com a ajuda de blocos na sessão Design, puxando e arrastando os componentes para a área de layout, dessa forma o código é gerado no arquivo de layout automaticamente.

A camada de controle é o componente fundamental de estrutura que define o aplicativo. É codificado em Java e rege os comportamentos e ações presentes no aplicativo.

Há quatro tipos diferentes de componentes de aplicativo. Cada tipo tem uma finalidade distinta e tem um ciclo de vida específico que define a forma pela qual o componente é criado e destruído.

As componentes são:

* Uma atividade é implementada como uma subclasse de **Activity**.
* Um serviço é implementado como uma subclasse de **Service**.
* Um provedor de conteúdo é implementado como uma subclasse de *ContentProvider* e precisa implementar um conjunto padrão de APIs que permitem a outros aplicativos realizar transações.
* Os receptores de transmissão são implementados como subclasses de *BroadcastReceiver*.

.1- Arquivo manifest; 2 - arquivo java; 3 - arquivos de layout.
image::imagens_final/img1.jpg[]

== 5. ESTRUTURA DO PROJETO

Basicamente a estrutura do aplicativo se dá em métodos essenciais já existentes das classes utilizadas que foram sobrepostos para fim específico da aplicação e métodos secundários auxiliares.

=== 5.1. MÉTODOS ESSENCIAIS SOBREPOSTOS DA ACTIVITY

O componente utilizado no experimento foi do tipo Activity. Basicamente em toda atividade há interação com o usuário, portanto torna-se mandatório a configuração de dados e tela a serem exibidos, dessa forma alguns métodos extremamente funcionais às atividades foram sobrepostos. São eles: **onDestroy()**, **onCreate(Bundle)**, **onPause()**, **onResume()**.
No método *onCreate(Bundle)* toda a atividade será inicializada e configurada, junto com as informações de tela. O conteúdo a ser mostrado ao usuário que está configurado no arquivo xml é setado no método **setContextView(view)**.

.Componente JavaCameraView no arquivo de Layout
image::imagens_final/view1.jpg[]

A ponte entre a classe nativa de câmera utilizada pelo sistema e a classe de câmera da biblioteca do OpenCV é identificada no arquivo de Layout através do identificador “R.id.java_surface_view” da componente **JavaCameraView**.

.Trechos destacados apresentam respectivamente a função onde o Layout é setado e JavaCameraView sendo alocado em variável.
image::imagens_final/oncreate.jpg[]

Os métodos **onPause()**, *onResume()* e *onDestroy()* funcionam respectivamente para pausar a aplicação quando o usuário o deixa em segundo plano, retornar a atividade que estava em segundo plano e destruir a atividade, terminando completamente a execução do aplicativo.

=== 5.2. MÉTODOS SOBREPOSTOS DA IMPLEMENTAÇÃO DE CLASSE OPENCV

Com o propósito de manipular os frames da filmagem antes deles serem mostrados ao usuário fez-se necessário que a classe principal implementasse a interface **CvCameraViewListener2**, que se comunica com o *JavaCameraView* e possibilita a obtenção dos quadros RGBA(quadros coloridos em canais vermelho, verde, azul e transparência, cuja variável é **mRgba**) obtidos e também a versão em escala de cinza(variável **mGray**) dos mesmos.
Essa interface oferece três métodos **onCameraViewStarted(int, int)**, **onCameraViewStopped() **e **onCameraFrame(CvCameraViewFrame)**. No primeiro método são passados por parâmetro os tamanhos de quadro para que as matrizes e variáveis possam ser inicializadas. Isso acontece quando o preview da câmera é iniciado.

.Imagem do código do método onCameraView e suas dependencias.
image::imagens_final/oncameraviewstarted.jpg[]

O segundo método libera as matrizes dos quadros e é chamada quando o preview não se faz mais necessário.
O terceiro método é onde as ações do aplicativo ocorrem. Tal função passa por parâmetro o **inputFrame**, que é o frame capturado via câmera do dispositivo e após manipulação retorna o frame para ser visualizado no preview pelo usuário na componente *JavaCameraView* localizado no Layout. Basicamente 90% do comportamento e ações do aplicativo advém da codificação inserida nesse método, que será discutida nas sessões seguintes.

=== 5.3. ANÁLISE DO CÓDIGO DE DETECÇÃO E RASTREIO DE FACES

Nessa sessão será discutido a detecção e rastreio de faces dos quadros capturados pela câmera traseira dos dispositivos. Os procedimentos serão apresentados em duas etapas, uma para detecção e outra para o rastreio, pois é dessa forma que o aplicativo funciona. O código para essas operações estão dentro do método **onCameraFrame**. 

==== 5.3.1. DETECÇÃO DE FACES

Antes de comentar sobre o código do método *onCameraFrame* é necessário relatar sobre a inicialização da classe abstrata **BaseLoaderCallback**, necessária por prover suporte entre o do gerenciador OpenCV(baixado no Google Play) e o as funções do aplicativo, basicamente essa classe declara um método de retorno para certificar que as bibliotecas do OpenCV estão disponíveis.
Na função *void onManagerConnected(final int)* caso o status seja de sucesso os classificadores Haar e LBP são inicializados(classificadores oferecidos pelo próprio OpenCV), caso haja algum erro de comunicação é emitido mensagem de falha. A imagem abaixo mostra apenas a inicialização do classificador de face(optou-se por classificador do tipo LBP para encontrar as faces por ser mais rápidos apesar de possuir um a taxa de erros um pouco maior), mas em sequência há a inicialização de classificadores de olhos, nariz e boca(esses classificadores são do tipo Haar).

.Trecho do código onde os classificadores são inicializados e a esquerda mostra a pasta onde o arquivo se localiza.
image::imagens_final/imag_classificador.jpg[]

Voltando ao início do método *onCameraFrame* é possível constatar que as variáveis de quadro colorido e em escala de cinza recebem informações dos frames capturados da variável **inputFrame**. A variável condicionante para detectar faces ou executar o fluxo óptico é **achouFace**. Caso tal variável seja falsa a detecção é iniciada no método void *detectMultiScale(Mat, MatOfRect, double, int, int, Size, Size)* pertencente aos classificadores.

Detalhes dos parâmetros:

* **Mat**: Matriz do tipo CV_8U, contendo a imagem onde os objetos serão detectados.
* **MatOfRect**: Vetor de objetos do tipo retângulo, onde cada um contem um objeto detectado.
* **double**:  Parâmetro de escala, especifica quanto a imagem é reduzida a cada iteração.
* **int**: Especifica a quantidade de retângulos vizinhos que cada candidato deve possui para retê-lo.
* **int**: Representa o método de identificação de objeto em cena(não é usado pelos classificadores novos).
* **Size**: Tamanho mínimo que a imagem pode ter. Abaixo disso são ignoradas.
* **Size**: Tamanho máximo que a imagem deve ter. Acima disso são ignoradas.

Dessa forma o código utilizado para achar faces na imagem em escala de cinza foi o seguinte: **cascade.detectMultiScale(mGray, faces, 1.15, 3, 1, new Size(100, 100)**, new Size(400, 400)). Podemos observar que faces menores que 100 por 100 pixels são ignorados, assim como maiores do que 400 por 400. O número de vizinhos escolhido foi 3 para que sejam reduzidas as detecções de não face. A escala é de 15%(1,15), número suficiente para o processo não ser demasiadamente demorado e não cresça de forma a perder muitas faces.
Os pontos que compõem os retângulos de faces encontradas são armazenadas na função **adicionaPontos(pontos, facesArray[i])**, que os salva na lista de objetos Point chamada pontos. A partir da área das faces encontradas é que são procurados os outros elementos faciais, mas toda a área não é levada em consideração, ela na verdade é reduzida ao retângulo de interesse que é retornada pelo método **Rect redimensionaROI(Rect, int)**, que recebe o retângulo da face e o código representante do elemento de redução(olho, nariz, boca etc). Dessa forma reduz-se esforço computacional e possíveis erros.

As reduções do retângulo feitas pelo método para cada código são:

* **BOCA**: Para X do P1 mantem o valor do X inicial mais vinte por cento da largura do retângulo vermelho, para o Y usa-se o Y inicial mais sessenta por cento da altura do retângulo vermelho. Para o X do P2 usa-se o X inicial mais oitenta por cento da largura do retângulo vermelho, para o Y do P2 usa-se o Y final do retângulo vermelho.
* **NARIZ**: Para X do P1 mantem o valor do X inicial mais vinte por cento da largura do retângulo vermelho, para o Y usa-se o Y inicial do retângulo vermelho mais vinte e cinco por cento da altura. Para o X do P2 usa-se o X inicial mais oitenta por cento da largura do retângulo vermelho, para o Y do P2 usa-se o Y inicial do retângulo vermelho mais oitenta por cento da altura.
* **OLHOS**: Para X do P1 mantem o valor do X inicial do retângulo vermelho, para o Y usa-se o Y inicial do retângulo vermelho mais vinte por cento da altura. Para o X do P2 usa-se o X final do retângulo vermelho, para o Y do P2 usa-se o Y inicial do retângulo vermelho mais sessenta por cento da altura.

.Trechos de interesse para a procura de elementos. 1- Área de interesse dos olhos; 2- Área de interesse do nariz; 3 - Área de interesse da boca.
image::imagens_final/rosto.jpg[]

Após a obtenção dos retângulos de interesse os elementos faciais são encontrados em uma função criada funcional aos 3 elementos chamada **void achaObjetosPorClassificador(Mat, Rect, CascadeClassifier, ArrayList<Point>, int, int, int)**, cujos parâmetros são: a matriz de referência(quadro em escala de cinza), o retângulo de interesse, o classificador a ser utilizado, a lista de pontos a serem armazenados dos retângulos achados, o código informando o que será buscado(nariz, olho, etc), e as configurações de tamanho mínimo em X e Y da imagem. No método há uma contagem para que sejam computados apenas 2 olhos e uma boca e nariz para cada face. Os pontos armazenados são os pontos centrais dos retângulos dos elementos de face, que coincidem com a pupila dos olhos, a ponta do nariz e o centro da boca.
Esse processo de “aprendizagem” se repete por 7 frames afim de que todos os elementos sejam encontrados. A partir do sétimo frame a variável *achouFace* passa a ter valor *true* e a parte do código executada passa a ser o de rastreamento. É possível verificar o trecho do código comentado nessa sessão abaixo.

.1 - Detecção de face; 2 - Adição dos pontos dos elementos do retângulo encontrado; 3 - Funções para encontrar nariz, olhos e boca.
image::imagens_final/codigo_detectMulti.jpg[]

==== 5.3.2. RASTREAMENTO DE FACES

O rastreio é feito quando alguma face é encontrada. Primeiramente há o teste para verificar se todos os elementos das faces foram encontrados(a quantidade de pontos considerada é de 8 pontos para cada face, 4 do bounding box do rosto e 4 dos elementos sociais), caso o resultado do teste seja falso as variáveis *achouFace* e *countFrames* são inicializadas e há uma nova procura por rostos. Caso o resultado seja verdadeiro há o prosseguimento do código com a inicialização da variável **features**, que armazena os pontos encontrados na detecção anterior para serem rastreados. O método que realiza o rastreamento é
**void calcOpticalFlowPyrLK(Mat, Mat, MatOfPoint2f, MatOfPoint2f, MatOfByte, MatOfFloat)**, que o faz usando o algoritmo de Lucas-Kanade através do cálculo do fluxo óptico para um conjunto de pontos esparsos usando o método das pirâmides.

Detalhe dos parâmetros:

* **Mat**: Matriz da imagem anterior.
* **Mat**: Matriz da imagem atual.
* **MatOfPoint2f**: Matriz de pontos em 2d, onde será verificado o fluxo ótico.
* **MatOfPoint2f**: Matriz de pontos em 2d com as novas posições.
* **MatOfByte**: Vetor de Status.
* **MatOfFloat**: Vetor de erros.

Dessa forma o método rastreia onde estão localizados os pontos de feature no quadro atual comparado ao quadro anterior. 
O algoritmo construído mostra pontos verdes, que correspondem ao feature atual, e vermelhos mostrando a feature de um quadro anterior. Ligando os pontos existe uma linha vermelha que demonstra o deslocamento dos pontos no espaço.
Ao fim do ciclo, os pontos são checados afim de verificar se o objeto rastreado saiu do frame, em caso positivo as variáveis que decidem a detecção de face são inicializadas com valores que permite novas buscas por rostos.
Na imagem abaixo pode-se observar trecho do código detalhado.

.Imagem do trecho de código do rastreamento. 1- Método de rastreio, retorna features nas novas posições; 2 - Desenho dos pontos e linhas das features no quadro colorido; 3 - Salvando o quadro atual para serve de quadro anterior na próxima iteração.
image::imagens_final/codigo_tracking.jpg[]

Ao fim de todas as operações, as posições calculadas dos pontos e retângulos encontrados são úteis para o desenho dos mesmos no quadro colorido, o qual é retornado ao fim do método *onCameraFrame* e por fim mostrado em tela para o usuário.


.Vídeo explicativo

video::u0EbygiKa1M[youtube]

== 6. CONCLUSÕES

A detecção de faces ocorreu de forma satisfatória e com poucos erros, porém é uma operação com elevado custo computacional, principalmente quando haviam muitos rostos em cena, de tal forma que não serve como fonte rastreadora ainda mais se levado em conta algoritmos que rodam em tempo real. A partir desse pensamento foi de vital importância a inclusão de outro algoritmo para o rastreamento após a detecção dos pontos de face. O algoritmo escolhido foi o de Lucas-Kanade que apesar de eficiente gera erros quando os pixels(features) rastreados se deslocam demasiadamente no espaço entre um quadro e outro. Apesar desses fatores e levando em conta a magnitude do projeto e o tempo necessário para a execução, os resultados foram proveitosos, pois na maioria das situações bem-sucedidas é notória a forma como os algoritmos trabalham juntos para achar e rastrear os rostos. Os erros encontrados, por sua vez, se transformam em cenários de base para possíveis estudos e desenvolvimento na criação de classificadores próprios, que garantem uma quantidade maior de quadros por segundo, ou uma melhor interação entre a câmera nativa e a câmera da biblioteca, assim como tratamento de erros para rastreamento indevido.

== 7. REFERÊNCIAS

KAPUR, Salil; THAKKAR, Nisarg. Mastering OpenCV Android Application Programming. UK: Packt Publishing, 2015.

HOWSE, Joseph. Android Application Programming with OpenCV. UK: Packt Publishing, 2013.
SANTOS, Túlio L. Detecção de faces através do algoritmo de Viola-Jones. COPPE/UFRJ, 2011. Disponível em:<http://http://www.academia.edu/9158427/Detec%C3%A7%C3%A3o_de_faces_atrav%C3%A9s_do_algoritmo_de_Viola-Jones>. Acesso em 15 de Junho de 2016.

CAMPOS, Filipe M. S de. Detecção e rastreamento de faces em vídeos -  Como detectar faces em vídeos?. Bit a Bit. 2011. Disponível em:<http://http://www.bitabit.eng.br/2011/02/21/como-detectar-faces-em-videos/>. Acesso em 15 de Junho de 2016.

WIKIPEDIA. Viola-Jones object detection framework. Wikipedia. Disponível em:<http://https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework>. Acesso em 15 de Junho de 2016.

CRUZ, Juliano E. C. Reconhecimento de Objetos em Imagens Orbitais com Uso de Abordagens do Tipo Descritor-Classificador. 2014. 107f. Dissertação de Mestrado em Computação Aplicada - Instituto nacional de Pesquisas Espaciais (INPE), São José dos Campos, 2014.  

BRITO, Agostinho. Processamento Digital de Imagens. Slide. Departamento de Engenharia da Computação e Automação, Universidade Federal do Rio Grande do Norte, 2016. Disponível em:<http://http://agostinhobritojr.github.io/cursos/pdi/fluxo.pdf>. Acesso em 15 de Junho de 2016.

AKTHAR, Imran. OpenCV-Android-FaceDectect-GoodFeature. Github, 2013. Disponível em:<http://https://github.com/crankdaworld/OpenCV-Android-FaceDetect-GoodFeature/blob/master/OpenCV-Android-FaceDetect-GoodFeature/face-detection/src/org/opencv/samples/fd/WorkingHeadPose.java>. Acesso em 15 de Junho de 2016.

GITHUB. Opencv. Github, 2015. Disponível em:<http://https://github.com/Itseez/opencv/blob/master/samples/cpp/lkdemo.cpp>. Acesso em 15 de Junho de 2016.

PUPIL LABS. Pupil - eye tracking platform. Github, 2016. Disponível em:<http://https://github.com/pupil-labs/pupil/blob/master/pupil_src/shared_modules/square_marker_detect.py>. Acesso em 15 de Junho de 2016.

HOSEK, Roman. Android eye detection and tracking with OpenCV. Disponível em:<http://http://romanhosek.cz/android-eye-detection-and-tracking-with-opencv/>. Acesso em 15 de Junho de 2016.

ANDROID DEVELOPERS. Api-Guides: App Components. Andorid. Disponível em:<http://https://developer.android.com/guide/components/index.html

== 8. ANEXOS


O arquivo JAVA pode ser encontrados nesse link: https://github.com/angelelouise/angelelouise.github.io/blob/master/arquivos/pdi/imagens_final/Arquivos%20principais/Arquivo%20java/FAceDetection.java[Link 1]

O arquivo Manifest nesse link:
https://github.com/angelelouise/angelelouise.github.io/blob/master/arquivos/pdi/imagens_final/Arquivos%20principais/Arquivo%20manifest/AndroidManifest.xml[Link 1]

Os arquivos de Layout nos links:
https://github.com/angelelouise/angelelouise.github.io/blob/master/arquivos/pdi/imagens_final/Arquivos%20principais/Arquivos%20layout/activity_face_detection.xml[Link 1]
https://github.com/angelelouise/angelelouise.github.io/blob/master/arquivos/pdi/imagens_final/Arquivos%20principais/Arquivos%20layout/content_face_detection.xml[Link 2]

link:index.html[Voltar]